{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "SLIDE_WINDOW_SIZE = 25  # size of the slide window 1 if default statefunction is chosen\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self, scaler = MinMaxScaler(), file=\"test.csv\" ,dir=\"./series/\", verbose=True):\n",
    "    super(CustomEnv, self).__init__()\n",
    "    self.filename = file\n",
    "    self.file = os.path.join(dir + self.filename)\n",
    "    self.cursor = -1\n",
    "    self.cursor_init = 0\n",
    "    self.isdone = False\n",
    "    self.statefunction = self.__get_state_q\n",
    "    self.rewardfunction = self.__get_reward_q\n",
    "    self.scaler = scaler\n",
    "\n",
    "    self.timeseries_labeled = pd.read_csv(os.path.join(dir + file), usecols=[1, 2], header=0, sep=\",\",\n",
    "                                          names=['value', 'anomaly'],\n",
    "                                          encoding=\"utf-8\")\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions:\n",
    "    self.action_space = spaces.Discrete(2)\n",
    "    # Example for using image as input:\n",
    "    self.observation_space = spaces.Box(low=0.0, high=1.0,\n",
    "                                        shape=(SLIDE_WINDOW_SIZE,), dtype=np.float32)\n",
    "    if verbose:\n",
    "        print(self.__str__())\n",
    "\n",
    "\n",
    "  # def step(self, action):\n",
    "  #   reward = self.__get_reward_q(self.timeseries_labeled, self.cursor, action)\n",
    "  #   state = self.__get_state_q(self.timeseries_labeled, self.cursor)\n",
    "  #   self.cursor += 1\n",
    "  #   state_ = self.__get_state_q(self.timeseries_labeled, self.cursor)\n",
    "  #   done = self.cursor >= len(self.timeseries_labeled) - 1\n",
    "  #   return state, reward, done, {}\n",
    "  def step(self, action):\n",
    "    reward = self.__get_reward_q(self.timeseries_labeled, self.cursor, action)\n",
    "    state = self.__get_state_q(self.timeseries_labeled, self.cursor)\n",
    "    self.cursor += 1\n",
    "    if self.cursor >= self.timeseries_labeled['value'].size:\n",
    "        done = True\n",
    "        next_state = []\n",
    "    else:\n",
    "        done = False\n",
    "        next_state = self.statefunction(self.timeseries_labeled, self.cursor)\n",
    "    return state, reward, done, {}\n",
    "\n",
    "  def reset(self):\n",
    "    self.cursor = self.cursor_init\n",
    "    self.isdone = False\n",
    "    self.normalize_timeseries()\n",
    "    init_state = self.statefunction(self.timeseries_labeled, self.cursor)\n",
    "    return init_state\n",
    "\n",
    "  def render(self, mode='human'):\n",
    "    pass\n",
    "\n",
    "  def close (self):\n",
    "    pass\n",
    "\n",
    "  def __get_state_q(self, timeseries, cursor):\n",
    "    \"\"\"\n",
    "    :param timeseries_cursor: the position where in the TimeSeries we are currently\n",
    "    :return: The Value of the current position, states with the same value are treated the same way\n",
    "    \"\"\"\n",
    "    return np.asarray([np.float64(timeseries['value'][cursor])])\n",
    "\n",
    "  def __get_reward_q(self, timeseries, cursor, action):\n",
    "    \"\"\"\n",
    "    :param timeseries_cursor: the position where in the TimeSeries we are currently\n",
    "    :param action: the chosen action (the label we put on the state, Anomaly or Normal)\n",
    "    :return: Rewards shaped inside the Config File\n",
    "    \"\"\"\n",
    "    if action == timeseries['anomaly'][cursor]:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "  def __str__(self):\n",
    "    \"\"\"\n",
    "    :return: String Representation of the TimeSeriesEnvironment Class, mainly for debug information\n",
    "    \"\"\"\n",
    "    return \"TimeSeries from: {}\\n Header(labeled):\\n {} \\nRows:\\n \" \\\n",
    "           \"{}\\nMeanValue:\\n {}\\nMaxValue:\\n {}\\nMinValue:\\n {}\".format(\n",
    "        self.filename,\n",
    "        self.timeseries_labeled.head(\n",
    "            3),\n",
    "        self.timeseries_labeled.shape[0],\n",
    "        round(self.timeseries_labeled[\"value\"].mean(), 2),\n",
    "        round(self.timeseries_labeled[\"value\"].max(), 2),\n",
    "        round(self.timeseries_labeled[\"value\"].min(), 2))\n",
    "\n",
    "  def normalize_timeseries(self):\n",
    "    self.timeseries_labeled[\"value\"] = self.scaler.fit_transform(self.timeseries_labeled[[\"value\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if environment is legit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeries from: test.csv\n",
      " Header(labeled):\n",
      "    value  anomaly\n",
      "0      0        0\n",
      "1      0        0\n",
      "2      4        0 \n",
      "Rows:\n",
      " 1424\n",
      "MeanValue:\n",
      " 40.39\n",
      "MaxValue:\n",
      " 452\n",
      "MinValue:\n",
      " 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The observation returned by the `reset()` method does not match the given observation space",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4efb0bdb72d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# It will check your custom environment and output additional warnings if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;31m# ============ Check the returned values ===============\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[0m_check_returned_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;31m# ==== Check the render method and the declared render modes ====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_returned_values\u001b[1;34m(env, observation_space, action_space)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0m_check_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"reset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;31m# Sample a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_obs\u001b[1;34m(obs, observation_space, method_name)\u001b[0m\n\u001b[0;32m    100\u001b[0m         )\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     assert observation_space.contains(\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     ), \"The observation returned by the `{}()` method does not match the given observation space\".format(method_name)\n",
      "\u001b[1;31mAssertionError\u001b[0m: The observation returned by the `reset()` method does not match the given observation space"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define and Train the agent\n",
    "model = PPO('MlpPolicy', env, n_steps=2048, batch_size=24, n_epochs=10, gamma=0.9,)\n",
    "#model = DQN(\"MlpPolicy\", env)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom evaluator\n",
    "\n",
    "def evaluate(model, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    env = model.get_env()\n",
    "    # Using an Array to track all rewards over all episodes\n",
    "    all_episode_rewards = []\n",
    "    all_episode_actions = []\n",
    "    for i in range(num_episodes):\n",
    "        # testing for each episode on complete run until the environment is done\n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        done = False\n",
    "        # get the first observation out of the environment\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_actions.append(int(action))\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_actions.append(episode_actions)\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "        best_episode_idx = np.argmax(all_episode_rewards)\n",
    "        best_episode_actions = all_episode_actions[best_episode_idx]\n",
    "\n",
    "    print(\"Maximum Reward: \", np.max(all_episode_rewards),\n",
    "          \"\\nAverage Reward: \", np.mean(all_episode_rewards), \"\\nEpisodes: \", num_episodes)\n",
    "    plot_result(model.get_env(), best_episode_actions)\n",
    "\n",
    "def plot_result(env, actions):\n",
    "    series = pd.DataFrame(env.get_attr(\"timeseries_labeled\")[0])\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(series.index , actions, label=\"Actions\", linestyle=\"solid\")\n",
    "    plt.plot(series.index , series[\"anomaly\"] , label=\"True Label\", linestyle=\"dotted\")\n",
    "    plt.plot(series.index , series[\"value\"] , label=\"Series\", linestyle=\"dashed\")\n",
    "    plt.legend()\n",
    "    plt.ylabel('Reward Sum')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our agent has too few information in its state to approximate the correct value function\n",
    "therefore we are trying to increase the state information by introducing a sliding window state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SlideWindowStateFuc(timeseries, timeseries_cursor, timeseries_states=None, action=None):\n",
    "    if timeseries_cursor >= SLIDE_WINDOW_SIZE:\n",
    "        return [timeseries['value'][i + 1]\n",
    "                for i in range(timeseries_cursor - SLIDE_WINDOW_SIZE, timeseries_cursor)]\n",
    "    else:\n",
    "        return np.zeros(SLIDE_WINDOW_SIZE)\n",
    "\n",
    "\n",
    "def SlideWindowRewardFuc(timeseries, timeseries_cursor, action):\n",
    "    if timeseries_cursor >= SLIDE_WINDOW_SIZE:\n",
    "        sum_anomaly = np.sum(timeseries['anomaly']\n",
    "                             [timeseries_cursor - SLIDE_WINDOW_SIZE + 1:timeseries_cursor + 1])\n",
    "        if sum_anomaly == 0:\n",
    "            if action == 0:\n",
    "                return 1  # 0.1      # true negative\n",
    "            else:\n",
    "                return -1  # 0.5     # false positive, error alarm\n",
    "\n",
    "        if sum_anomaly > 0:\n",
    "            if action == 0:\n",
    "                return -5  # false negative, miss alarm\n",
    "            else:\n",
    "                return 5  # 10      # true positive\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# now we try again by setting our new functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeries from: test.csv\n",
      " Header(labeled):\n",
      "    value  anomaly\n",
      "0      0        0\n",
      "1      0        0\n",
      "2      4        0 \n",
      "Rows:\n",
      " 1424\n",
      "MeanValue:\n",
      " 40.39\n",
      "MaxValue:\n",
      " 452\n",
      "MinValue:\n",
      " 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-518afba055a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mexploration_final_eps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_eval_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True)\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    223\u001b[0m     ) -> OffPolicyAlgorithm:\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         return super(DQN, self).learn(\n\u001b[0m\u001b[0;32m    226\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m             rollout = self.collect_rollouts(\n\u001b[0m\u001b[0;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_episodes_rollout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(self, env, callback, n_episodes, n_steps, action_noise, learning_starts, replay_buffer, log_interval)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[1;31m# Select action randomly or according to policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sample_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_starts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_noise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[1;31m# Rescale and perform action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\u001b[0m in \u001b[0;36m_sample_action\u001b[1;34m(self, learning_starts, action_noise)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[1;31m# we assume that the policy uses tanh to scale the action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[1;31m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0munscaled_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;31m# Rescale the action from [low, high] to [-1, 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\common\\policies.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m             \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m         \u001b[1;31m# Convert to numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\dqn\\policies.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\stable_baselines3\\dqn\\policies.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# Greedy action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "env.__setattr__(\"statefunction\", SlideWindowStateFuc)\n",
    "env.__setattr__(\"rewardfunction\", SlideWindowRewardFuc)\n",
    "#env.__setattr__(\"cursor_init\", 25)\n",
    "# check the env\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "#model = PPO('MlpPolicy', env, learning_rate=0.01, n_steps=2048, batch_size=48, n_epochs=2,\n",
    "#            gamma=0.9, gae_lambda=0.9,use_sde=False,\n",
    "#            clip_range=0.2, clip_range_vf=None, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5,)\n",
    "model = DQN(\"MlpPolicy\", env, learning_rate=0.001, buffer_size=50000, learning_starts=5000,\n",
    "            batch_size=SLIDE_WINDOW_SIZE, tau=1.0, gamma=0.9, train_freq=4, gradient_steps=1,\n",
    "            n_episodes_rollout=- 1, optimize_memory_usage=False, #\n",
    "            target_update_interval=100, exploration_fraction=0.1, exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.0, max_grad_norm=10, tensorboard_log=None, create_eval_env=False,\n",
    "            policy_kwargs=None, verbose=1, seed=None, device='gpu', _init_setup_model=True)\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.__setattr__(\"cursor_init\", 0)\n",
    "evaluate(model, num_episodes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pythonProject)",
   "language": "python",
   "name": "pycharm-bd005cd4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
