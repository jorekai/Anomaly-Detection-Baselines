{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import plotly.graph_objects as go\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from collections import deque\n",
    "\n",
    "SLIDE_WINDOW_SIZE = 1  # size of the slide window 1 if default statefunction is chosen\n",
    "\n",
    "def defaultState(timeseries, cursor, action):\n",
    "    \"\"\"\n",
    "    :param timeseries:\n",
    "    :param cursor: the position where in the TimeSeries we are currently\n",
    "    :return: The Value of the current position, states with the same value are treated the same way\n",
    "    \"\"\"\n",
    "    state = np.asarray([np.float64(timeseries['value'][cursor]), 1 if action == 1 else 0])\n",
    "    return state\n",
    "\n",
    "def defaultReward(state, timeseries, cursor, action, path=None):\n",
    "    if state[1] == 1 and timeseries['anomaly'][cursor] == 1:\n",
    "        if action == 1:\n",
    "            return 1\n",
    "        if action == 0:\n",
    "            return -1\n",
    "    if state[1] == 1 and timeseries['anomaly'][cursor] == 0:\n",
    "        if action == 1:\n",
    "            return -1\n",
    "        if action == 0:\n",
    "            return 1\n",
    "    if state[1] == 0 and timeseries['anomaly'][cursor] == 1:\n",
    "        if action == 1:\n",
    "            return 1\n",
    "        if action == 0:\n",
    "            return -1\n",
    "    if state[1] == 0 and timeseries['anomaly'][cursor] == 0:\n",
    "        if action == 1:\n",
    "            return -1\n",
    "        if action == 0:\n",
    "            return 1\n",
    "\n",
    "def SlideWindowStateFuc(timeseries, timeseries_cursor):\n",
    "    if timeseries_cursor >= SLIDE_WINDOW_SIZE:\n",
    "        return [timeseries['value'][i + 1]\n",
    "                for i in range(timeseries_cursor - SLIDE_WINDOW_SIZE, timeseries_cursor)]\n",
    "    else:\n",
    "        return np.zeros(SLIDE_WINDOW_SIZE)\n",
    "\n",
    "def SlideWindowRewardFuc(timeseries, timeseries_cursor, action, path=None):\n",
    "    p = np.array(path)\n",
    "    window = np.array(timeseries['anomaly'][timeseries_cursor - SLIDE_WINDOW_SIZE + 1:timeseries_cursor + 1])\n",
    "    print(window)\n",
    "    print(p)\n",
    "    if timeseries_cursor >= SLIDE_WINDOW_SIZE:\n",
    "        if np.array_equal(p,window):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self, statefunction=defaultState, rewardfunction=defaultReward, scaler = MinMaxScaler(), file=\"test.csv\" ,dir=\"./series/\", verbose=True):\n",
    "    super(CustomEnv, self).__init__()\n",
    "    self.filename = file\n",
    "    self.file = os.path.join(dir + self.filename)\n",
    "    self.cursor = -1\n",
    "    self.cursor_init = 0\n",
    "    self.statefunction = statefunction\n",
    "    self.rewardfunction = rewardfunction\n",
    "    self.scaler = scaler\n",
    "    self.actions = []\n",
    "    self.figure = go.FigureWidget()\n",
    "    self.path = deque([], maxlen=SLIDE_WINDOW_SIZE)\n",
    "\n",
    "    self.timeseries_labeled = pd.read_csv(os.path.join(dir + file), usecols=[1, 2], header=0, sep=\",\",\n",
    "                                          names=['value', 'anomaly'],\n",
    "                                          encoding=\"utf-8\")\n",
    "    self.action_space = spaces.Discrete(2)\n",
    "    self.observation_space = spaces.Box(low=0.0, high=1.0,\n",
    "                                        shape=(2,), dtype=np.float32)\n",
    "    if verbose:\n",
    "        print(self.__str__())\n",
    "\n",
    "  def step(self, action):\n",
    "    if len(self.path) >= self.path.maxlen:\n",
    "        oldest = self.path.pop()\n",
    "    if len(self.path) < self.path.maxlen and self.cursor >= SLIDE_WINDOW_SIZE:\n",
    "        self.path.appendleft(action)\n",
    "    else:\n",
    "        self.path.appendleft(0)\n",
    "    state = self.statefunction(self.timeseries_labeled, self.cursor, action)\n",
    "    reward = self.rewardfunction(state, self.timeseries_labeled, self.cursor, action, self.path)\n",
    "    self.actions.append(action)\n",
    "    self.cursor += 1\n",
    "    if self.cursor >= self.timeseries_labeled['value'].size:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return state, reward, done, {}\n",
    "\n",
    "  def reset(self):\n",
    "    self.cursor = self.cursor_init\n",
    "    self.actions = []\n",
    "    self.path.clear()\n",
    "    self.normalize_timeseries()\n",
    "    init_state = self.statefunction(self.timeseries_labeled, self.cursor, 0)\n",
    "    return init_state\n",
    "\n",
    "  def render(self, mode='human'):\n",
    "    if self.cursor == 1:\n",
    "        self.figure.add_scatter()\n",
    "        self.figure.add_scatter()\n",
    "        self.figure.add_scatter()\n",
    "        self.figure.show()\n",
    "    if self.cursor > 1:\n",
    "        series = pd.DataFrame(self.timeseries_labeled).iloc[:self.cursor]\n",
    "        series[\"actions\"] = self.actions[:self.cursor]\n",
    "        with self.figure.batch_update():\n",
    "            self.figure.data[0].y = series[\"actions\"]\n",
    "            self.figure.data[0].name = \"Actions\"\n",
    "            self.figure.data[1].y = series[\"value\"]\n",
    "            self.figure.data[1].name = \"Value\"\n",
    "            self.figure.data[2].y = series[\"anomaly\"]\n",
    "            self.figure.data[2].name = \"Anomaly\"\n",
    "\n",
    "  def close (self):\n",
    "    pass\n",
    "\n",
    "  def __str__(self):\n",
    "    \"\"\"\n",
    "    :return: String Representation of the TimeSeriesEnvironment Class, mainly for debug information\n",
    "    \"\"\"\n",
    "    return \"TimeSeries from: {}\\n Header(labeled):\\n {} \\nRows:\\n \" \\\n",
    "           \"{}\\nMeanValue:\\n {}\\nMaxValue:\\n {}\\nMinValue:\\n {}\".format(\n",
    "        self.filename,\n",
    "        self.timeseries_labeled.head(\n",
    "            3),\n",
    "        self.timeseries_labeled.shape[0],\n",
    "        round(self.timeseries_labeled[\"value\"].mean(), 2),\n",
    "        round(self.timeseries_labeled[\"value\"].max(), 2),\n",
    "        round(self.timeseries_labeled[\"value\"].min(), 2))\n",
    "\n",
    "  def normalize_timeseries(self):\n",
    "    self.timeseries_labeled[\"value\"] = self.scaler.fit_transform(self.timeseries_labeled[[\"value\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if environment is legit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = CustomEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, num_episodes=1):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    env = model.get_env()\n",
    "    # Using an Array to track all rewards over all episodes\n",
    "    all_episode_rewards = []\n",
    "    all_episode_actions = []\n",
    "    for i in range(num_episodes):\n",
    "        # testing for each episode on complete run until the environment is done\n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        done = False\n",
    "        # get the first observation out of the environment\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_actions.append(int(action))\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_actions.append(episode_actions)\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "        best_episode_idx = np.argmax(all_episode_rewards)\n",
    "        best_episode_actions = all_episode_actions[best_episode_idx]\n",
    "\n",
    "    print(\"Maximum Reward: \", np.max(all_episode_rewards),\n",
    "          \"\\nAverage Reward: \", np.mean(all_episode_rewards), \"\\n TestEpisodes: \", num_episodes)\n",
    "    plot_result(model.get_env(), best_episode_actions)\n",
    "\n",
    "def plot_result(env, actions):\n",
    "    series = pd.DataFrame(env.get_attr(\"timeseries_labeled\")[0])\n",
    "    series[\"actions\"] = actions\n",
    "    fig = px.line(series, x=series.index, y=[\"actions\",\"anomaly\",\"value\"])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our agent has too few information in its state to approximate the correct value function\n",
    "therefore we are trying to increase the state information by introducing a sliding window state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log = \"./dqn_tensorboard/\"\n",
    "model = DQN(\"MlpPolicy\", env,  learning_rate=0.001, buffer_size=50000, learning_starts=5000,\n",
    "            batch_size=SLIDE_WINDOW_SIZE, tau=1.0, gamma=0.99, train_freq=4, gradient_steps=10, n_episodes_rollout=- 1,\n",
    "            optimize_memory_usage=False, target_update_interval=10000, exploration_fraction=0.1,\n",
    "            exploration_initial_eps=1.0, exploration_final_eps=0.0, max_grad_norm=10, tensorboard_log=None,\n",
    "            create_eval_env=False,\n",
    "            policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True)\n",
    "model.learn(total_timesteps=10000, tb_log_name=\"first_run\")\n",
    "#evaluate(model, num_episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still observe the behavior, that our agent cannot detect hard cuts in falling anomalies as these\n",
    "are basically not detectable with the current state representation.\n",
    "\n",
    "Next we will try out the Binary State Function collecting all States in our Trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for i in range(10):\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pythonProject)",
   "language": "python",
   "name": "pycharm-bd005cd4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
