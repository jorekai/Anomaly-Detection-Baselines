{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-23bd67a13156>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from collections import deque\n",
    "\n",
    "SLIDE_WINDOW_SIZE = 1  # size of the slide window 1 if default statefunction is chosen\n",
    "\n",
    "def defaultState(timeseries, cursor):\n",
    "  \"\"\"\n",
    "  :param timeseries:\n",
    "  :param cursor: the position where in the TimeSeries we are currently\n",
    "  :return: The Value of the current position, states with the same value are treated the same way\n",
    "  \"\"\"\n",
    "  return np.asarray([np.float64(timeseries['value'][cursor])])\n",
    "\n",
    "def defaultReward(timeseries, cursor, action, path=None):\n",
    "  \"\"\"\n",
    "  :param timeseries:\n",
    "  :param cursor: the position where in the TimeSeries we are currently\n",
    "  :param action: the chosen action (the label we put on the state, Anomaly or Normal)\n",
    "  :return: Rewards shaped inside the Config File\n",
    "  \"\"\"\n",
    "  if action == timeseries['anomaly'][cursor]:\n",
    "      return 1\n",
    "  else:\n",
    "      return -1\n",
    "\n",
    "def SlideWindowStateFuc(timeseries, timeseries_cursor):\n",
    "    if timeseries_cursor >= SLIDE_WINDOW_SIZE:\n",
    "        return [timeseries['value'][i + 1]\n",
    "                for i in range(timeseries_cursor - SLIDE_WINDOW_SIZE, timeseries_cursor)]\n",
    "    else:\n",
    "        return np.zeros(SLIDE_WINDOW_SIZE)\n",
    "\n",
    "def SlideWindowRewardFuc(timeseries, timeseries_cursor, action, path=None):\n",
    "    p = np.array(path)\n",
    "    window = np.array(timeseries['anomaly'][timeseries_cursor - SLIDE_WINDOW_SIZE + 1:timeseries_cursor + 1])\n",
    "    print(np.count_nonzero(p==window))\n",
    "    if timeseries_cursor >= SLIDE_WINDOW_SIZE:\n",
    "        sum_anomaly = np.sum(window)\n",
    "        if sum_anomaly == 0:\n",
    "            if action == 0:\n",
    "                return 1  # 0.1      # true negative\n",
    "            else:\n",
    "                return -1  # 0.5     # false positive, error alarm\n",
    "\n",
    "        if sum_anomaly > 0:\n",
    "            if action == 0:\n",
    "                return -1  # false negative, miss alarm\n",
    "            else:\n",
    "                return 1  # 10      # true positive\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self, statefunction=defaultState, rewardfunction=defaultReward, scaler = MinMaxScaler(), file=\"test.csv\" ,dir=\"./series/\", verbose=True):\n",
    "    super(CustomEnv, self).__init__()\n",
    "    self.filename = file\n",
    "    self.file = os.path.join(dir + self.filename)\n",
    "    self.cursor = -1\n",
    "    self.cursor_init = 0\n",
    "    self.statefunction = statefunction\n",
    "    self.rewardfunction = rewardfunction\n",
    "    self.scaler = scaler\n",
    "    self.path = deque([], maxlen=SLIDE_WINDOW_SIZE)\n",
    "\n",
    "    self.timeseries_labeled = pd.read_csv(os.path.join(dir + file), usecols=[1, 2], header=0, sep=\",\",\n",
    "                                          names=['value', 'anomaly'],\n",
    "                                          encoding=\"utf-8\")\n",
    "    self.action_space = spaces.Discrete(2)\n",
    "    self.observation_space = spaces.Box(low=0.0, high=1.0,\n",
    "                                        shape=(SLIDE_WINDOW_SIZE,), dtype=np.float32)\n",
    "    if verbose:\n",
    "        print(self.__str__())\n",
    "\n",
    "  def step(self, action):\n",
    "    if len(self.path) >= self.path.maxlen:\n",
    "        # just saving to oldest maybe useful later on\n",
    "        oldest = self.path.pop()\n",
    "    if len(self.path) < self.path.maxlen:\n",
    "        self.path.appendleft(action)\n",
    "    reward = self.rewardfunction(self.timeseries_labeled, self.cursor, action, self.path)\n",
    "    state = self.statefunction(self.timeseries_labeled, self.cursor)\n",
    "    self.cursor += 1\n",
    "    if self.cursor >= self.timeseries_labeled['value'].size:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return state, reward, done, {}\n",
    "\n",
    "  def reset(self):\n",
    "    self.cursor = self.cursor_init\n",
    "    self.path.clear()\n",
    "    self.normalize_timeseries()\n",
    "    init_state = self.statefunction(self.timeseries_labeled, self.cursor)\n",
    "    return init_state\n",
    "\n",
    "  def render(self, mode='human'):\n",
    "    pass\n",
    "\n",
    "  def close (self):\n",
    "    pass\n",
    "\n",
    "  def __str__(self):\n",
    "    \"\"\"\n",
    "    :return: String Representation of the TimeSeriesEnvironment Class, mainly for debug information\n",
    "    \"\"\"\n",
    "    return \"TimeSeries from: {}\\n Header(labeled):\\n {} \\nRows:\\n \" \\\n",
    "           \"{}\\nMeanValue:\\n {}\\nMaxValue:\\n {}\\nMinValue:\\n {}\".format(\n",
    "        self.filename,\n",
    "        self.timeseries_labeled.head(\n",
    "            3),\n",
    "        self.timeseries_labeled.shape[0],\n",
    "        round(self.timeseries_labeled[\"value\"].mean(), 2),\n",
    "        round(self.timeseries_labeled[\"value\"].max(), 2),\n",
    "        round(self.timeseries_labeled[\"value\"].min(), 2))\n",
    "\n",
    "  def normalize_timeseries(self):\n",
    "    self.timeseries_labeled[\"value\"] = self.scaler.fit_transform(self.timeseries_labeled[[\"value\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if environment is legit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = CustomEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define and Train the agent\n",
    "model = PPO('MlpPolicy', env, n_steps=2048, batch_size=24, n_epochs=10, gamma=0.9,)\n",
    "#model = DQN(\"MlpPolicy\", env)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom evaluator\n",
    "\n",
    "def evaluate(model, num_episodes=1):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    env = model.get_env()\n",
    "    # Using an Array to track all rewards over all episodes\n",
    "    all_episode_rewards = []\n",
    "    all_episode_actions = []\n",
    "    for i in range(num_episodes):\n",
    "        # testing for each episode on complete run until the environment is done\n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        done = False\n",
    "        # get the first observation out of the environment\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_actions.append(int(action))\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_actions.append(episode_actions)\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "        best_episode_idx = np.argmax(all_episode_rewards)\n",
    "        best_episode_actions = all_episode_actions[best_episode_idx]\n",
    "\n",
    "    print(\"Maximum Reward: \", np.max(all_episode_rewards),\n",
    "          \"\\nAverage Reward: \", np.mean(all_episode_rewards), \"\\n TestEpisodes: \", num_episodes)\n",
    "    plot_result(model.get_env(), best_episode_actions)\n",
    "\n",
    "def plot_result(env, actions):\n",
    "    series = pd.DataFrame(env.get_attr(\"timeseries_labeled\")[0])\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(series.index , actions, label=\"Actions\", linestyle=\"solid\")\n",
    "    plt.plot(series.index , series[\"anomaly\"] , label=\"True Label\", linestyle=\"dotted\")\n",
    "    plt.plot(series.index , series[\"value\"] , label=\"Series\", linestyle=\"dashed\")\n",
    "    plt.legend()\n",
    "    plt.ylabel('Reward Sum')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our agent has too few information in its state to approximate the correct value function\n",
    "therefore we are trying to increase the state information by introducing a sliding window state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SLIDE_WINDOW_SIZE = 25  # size of the slide window 1 if default statefunction is chosen\n",
    "env = CustomEnv(statefunction=SlideWindowStateFuc, rewardfunction=SlideWindowRewardFuc)\n",
    "check_env(env)\n",
    "# check the env\n",
    "log = \"./dqn_tensorboard/\"\n",
    "model = DQN(\"MlpPolicy\", env,  learning_rate=0.001, buffer_size=50000, learning_starts=5000,\n",
    "            batch_size=SLIDE_WINDOW_SIZE, tau=1.0, gamma=0.99, train_freq=4, gradient_steps=1, n_episodes_rollout=- 1,\n",
    "            optimize_memory_usage=False, target_update_interval=10000, exploration_fraction=0.1,\n",
    "            exploration_initial_eps=1.0, exploration_final_eps=0.0, max_grad_norm=10, tensorboard_log=None,\n",
    "            create_eval_env=False,\n",
    "            policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True)\n",
    "model.learn(total_timesteps=10000, tb_log_name=\"first_run\")\n",
    "evaluate(model, num_episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can still observe the behavior, that our agent cannot detect hard cuts in falling anomalies as these\n",
    "are basically not detectable with the current state representation.\n",
    "\n",
    "Next we will try out the Binary State Function collecting all States in our Trace\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
